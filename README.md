1) 경사하강법의 유도

등산의 경우 산 꼭대기에서 최저점으로 하산하려면 비탈길을 타고 계속 내려오면 됩니다. 이와 마찬가지로 비용함수도 경사(기울기)가 아래로 향하는 방향으로 계속 이동하다보면 비용함수의 최저점에 도달하게 됩니다.


<img width="374" height="250" alt="Image" src="https://github.com/user-attachments/assets/c33418e6-e454-486c-b697-bfee802f0c20" />

 

초평면에 대한 기울기는 그라디언트(gradient, ∇)라고 합니다. 비용함수의 독립변수 xi 가 그라디언트에 비례하는 값만큼 이동한 경우 그라디언트 방향과 x의 이동방향은 기호가 반대입니다. 위에 그래프를 보면 x가 음수인 경우에는 그라디언트가 (-) 방향이며, x는 (+) 방향으로 이동하게 됩니다. 반대로 x가 양수라면 그라디언트의 방향은 (+) 이며, x는 (-) 방향으로 이동하게 됩니다. 식으로 표현하면 다음과 같습니다.

xi +1 = xi - 그라디언트 방향 * 이동거리

이동거리는 그라디언트에 비례하므로 그라디언트가 크면 x는 크게 증가하고 그라디언트가 작으면 x는 작게 증가합니다. 또한 학습률(learn rate)라고 불리는 그라디언트에 비례하는 값 α 에 의해 이동거리를 조절할 수 있으므로 다음과 같은 식으로 표현됩니다.

xi +1 = xi - α∇f(xi) 

그라디언트는 미분값이므로 다음과 같이 표현할 수도 있습니다.

단층 퍼셉트론 알고리즘

1) 가중치와 바이어스 가중치를 -0.5와 0.5 사이의 임의의 값으로, 바이어스 입력값을 임의의 값으로 초기화
2) 하나의 학습 벡터에 대한 출력층 뉴런의 net값을 계산
3) 활성함수를 통해 계산된 net값으로부터 뉴런의 실제 출력값을 계산
4-1) 뉴런의 출력값과 목표값의 차이가 허용 오차보다 작으면 [5]로 이동
4-2) 뉴런의 출력값과 목표값의 차이가 허용 오차보다 크면 학습을 진행
5-1) 현재 학습 벡터가 마지막 학습 벡터가 아니면, 현재 학습 벡터를 다음 학습 벡터로 설정하고 [2]로 이동하여 반복
5-2-1) 현재 학습 벡터가 마지막 학습 벡터이고, 모든 학습 벡터에 대해 출력값과 목표값이 허용 오차보다 작으면
 알고리즘을 종료
5-2-2) 현재 학습 벡터가 마지막 학습 벡터이지만 출력값과 목표값이 허용 오차보다 큰 학습 벡터가 존재하면, 
현재 학습 벡터를 처음 학습 벡터로 설정하고 [2]로 이동하여 반복

미분을 통해 계수를 유도하는 이유는 기울기를 구하기 위해서이다. 
함수의 그래프에서 미분값은 특정 지점에서의 접선의 기울기를 의미한다. 
좀 더 빨리 값을 얻기 위해서 기울기에 학습 비율(learning rate)을 곱하는 것이 일반적이다.


